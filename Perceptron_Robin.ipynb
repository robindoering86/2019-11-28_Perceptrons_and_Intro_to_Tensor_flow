{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron, Activation function, Error function, and need for weights and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "Perceptrons were designed to mimic the way neurons in the human brain work. They take different inputs and apply weights to the inputs. Then, all weighted are summed and passed to an activation function, which can give a binary  ('yes' / 'no' , 1 / 0) or continous output. The way a perceptron works can be summarized into the four following steps.\n",
    "\n",
    "1. Enter input values\n",
    "2. Apply weights and bias\n",
    "3. Sum over als weighted inputs\n",
    "4. Calculate output with activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "\n",
    "An activation function is used to generate the output of a neuron. It takes the weighted sum of all inputs and a bias value as input and then returns either an continous (probability) or discrete (classification) output.\n",
    "\n",
    "Examples for commonly used activation functions are:\n",
    "* sigmoid\n",
    "* ReLU\n",
    "* Heaviside (step function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error function\n",
    "\n",
    "As for other ML applications (e.g. linear regression), the error (or cost, lost) function is used to evaluate the quality of a model. It compares the prediction the model makes for the test data point to the respective true value for that data point. The goal is to minimize the error of the test data set. In neural networks the output of the error function is used to optimize the weights and bias values for every single neuron of every single layer. This process is called backpropagation, where, starting from the output layer, the weights of the individual neurons are optimized in reversed fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and bias\n",
    "\n",
    "The weights are the values that are applied to the inputs of the neurons of ever single layer. Hence they are one of the two decisive factors (the other being the activation function) as to whether a neuron is actived. A bias can be applied, to aggravate (typically by applying negative bias) or facilitate (typically by applying positive bias) that a specific neuron is activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
